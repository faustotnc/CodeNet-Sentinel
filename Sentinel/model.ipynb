{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    prev_dir = os.getcwd()\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Replace with correct location\n",
    "    %cd /content/drive/MyDrive/Colab Notebooks/CodeNet-Sentinel/Decoder\n",
    "\n",
    "    !pip install datasets transformers lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make our imports work because python relative imports suck\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "# Local Modules\n",
    "from Architecture import  Tokenizer, VOCAB_SIZE\n",
    "from Architecture.ModelConfig import ModelConfig\n",
    "from Architecture.Encoder import EncoderBlock, QnAConcatBlock\n",
    "from Architecture.SentinelTransformer import SentinelDataset, SentinelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = ModelConfig()\n",
    "\n",
    "# Config specific to the Decoder\n",
    "CONFIG.batch_size = 10\n",
    "CONFIG.grad_accumulation = 3\n",
    "\n",
    "torch.manual_seed(CONFIG.random_seed)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = SentinelDataset.load_from(\"./data/sentinel_data.pt\")\n",
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(full_dataset)\n",
    "\n",
    "test_size = int(CONFIG.test_split * total_size)\n",
    "val_size = int(CONFIG.val_split * (total_size - test_size))\n",
    "train_size = total_size - test_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(\"Training Dataset Size:\", len(train_dataset))\n",
    "print(\"Training Dataset Size:\", len(val_dataset))\n",
    "print(\"Training Dataset Size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Training Batches:\", len(train_loader))\n",
    "print(\"Number of Validation Batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(val_loader))\n",
    "print(sample_batch.keys(), \"\\n\")\n",
    "\n",
    "for key, item in sample_batch.items():\n",
    "    print(f\"{key}:\".ljust(32), item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately, the connection to Google Drive dies after a couple\n",
    "# of hours, crashing the entire notebook. To prevent any issues when\n",
    "# running the notebok in the background, we must unmount and make\n",
    "# the previous directory (`prev_dir`) our current directory.\n",
    "if 'google.colab' in sys.modules:\n",
    "    %cd $prev_dir\n",
    "    drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = SentinelModel(\n",
    "    encoder_block=EncoderBlock,\n",
    "    qna_concat_block=QnAConcatBlock,\n",
    "    n_layers=CONFIG.n_layers,\n",
    "    n_head=CONFIG.n_head,\n",
    "    n_dim=CONFIG.n_dim,\n",
    "    max_seq_len=CONFIG.max_seq_len,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_logits=len(full_dataset.status_values),\n",
    "    mlp_dropout=CONFIG.mlp_dropout,\n",
    "    learning_rate=CONFIG.learning_rate,\n",
    "    min_learning_rate=CONFIG.min_learning_rate,\n",
    "    weight_decay=CONFIG.weight_decay,\n",
    "    beta1=CONFIG.beta1,\n",
    "    beta2=CONFIG.beta2,\n",
    "    max_iters=len(train_loader) * CONFIG.num_epochs,\n",
    "    bias=CONFIG.bias\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "# print(\"compiling the model... \", end=\"\")\n",
    "# model = torch.compile(model, backend=\"cudagraphs\") # requires PyTorch 2.0\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "if CONFIG.wandb_log:\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=CONFIG.wandb_project_name,\n",
    "        name=CONFIG.wandb_run_name,\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "    # log gradients and model topology\n",
    "    # wandb_logger.watch(transformer)\n",
    "\n",
    "# class GenerateTextCallback(L.Callback):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.instructions_text = [\n",
    "#             \"[CLS]Write a javascript function that makes a get request to retrieve a song based on an api endpoint and a song id.[SEP]\",\n",
    "#             \"[CLS]rust[SEP]\",\n",
    "#             \"[CLS]Complete this python function to compute the determinant of a square matrix.\",\n",
    "#         ]\n",
    "\n",
    "#         self.tokenized_instructions = Tokenizer.batch_encode_plus(\n",
    "#             batch_text_or_text_pairs=self.instructions_text,\n",
    "#             truncation=True,\n",
    "#             max_length=CONFIG.max_instruct_len,\n",
    "#             padding=\"max_length\",\n",
    "#             return_attention_mask=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).to(\"cuda\")\n",
    "\n",
    "#         self.response_seed_text = [\n",
    "#             \"[BOS]\",\n",
    "#             \"[BOS]use crate::virtual_machine::{RuntimeResult, VirtualMachine};\\n\\nimpl VirtualMachine {\\n  /// Executes the instructions in a chunk of byte code\\n  pub(crate) fn run(&mut self) -> RuntimeResult {\\n    loop {\\n      let instruction = self.next_op_code();\",\n",
    "#             \"[BOS]import numpy as np\\n\\ndef compute_determinant(matrix):\\n\"\n",
    "#         ]\n",
    "\n",
    "#         self.tokenized_response_seeds = [\n",
    "#             Tokenizer.encode(\n",
    "#                 self.response_seed_text[0],\n",
    "#                 return_tensors=\"pt\"\n",
    "#             ).to(\"cuda\"),\n",
    "\n",
    "#             Tokenizer.encode(\n",
    "#                 self.response_seed_text[1],\n",
    "#                 return_tensors=\"pt\"\n",
    "#             ).to(\"cuda\"),\n",
    "\n",
    "#             Tokenizer.encode(\n",
    "#                 self.response_seed_text[2],\n",
    "#                 return_tensors=\"pt\"\n",
    "#             ).to(\"cuda\")\n",
    "#         ]\n",
    "\n",
    "#     def on_validation_epoch_end(self, trainer, pl_module):\n",
    "#         pl_module.eval()\n",
    "\n",
    "#         generated_text = []\n",
    "#         for idx in range(len(self.tokenized_instructions[\"input_ids\"])):\n",
    "#             with torch.no_grad():\n",
    "#                 generated = pl_module._generate(\n",
    "#                     self.tokenized_instructions[\"input_ids\"][idx].unsqueeze(0),\n",
    "#                     ~(self.tokenized_instructions[\"attention_mask\"][idx].bool()).unsqueeze(0),\n",
    "#                     self.tokenized_response_seeds[idx],\n",
    "#                     max_new_tokens=1024\n",
    "#                 )\n",
    "\n",
    "#                 generated_text.append(Tokenizer.decode(generated[0]))\n",
    "\n",
    "#         # Log the generated text to W&B\n",
    "#         columns = [\"Instruction\", \"Response Seed\", \"Generated Response\"]\n",
    "#         data = list(zip(self.instructions_text, self.response_seed_text, generated_text))\n",
    "#         wandb_logger.log_text(key=\"Text Generation Samples\", columns=columns, data=data)\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "# text_gen_callback = GenerateTextCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trainer\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\"./checkpoints/\",\n",
    "    max_epochs=CONFIG.num_epochs,\n",
    "    # We specified that validation metrics are not logged at every \"validation step\", but rather\n",
    "    # at every \"validation epoch.\" This is different from the training metrics, which are logged\n",
    "    # at every training step and every training epoch. Note that validation steps are different from\n",
    "    # training step. The `log_every_n_steps` parameter accounts for this difference.\n",
    "    val_check_interval=CONFIG.log_interval,\n",
    "    # Because we have gradient accumulation, the training step is different from the global step.\n",
    "    # The global step is used to log the metrics at the interval we specify here, and is multiplied by\n",
    "    # the gradient accumulation steps. To align the validation logs with the training logs, we must\n",
    "    # divide the log interval by the gradient accumulation step. We further divide by 10, such that it\n",
    "    # logs the training loss 10 times in the same period it logs one validation loss.\n",
    "    log_every_n_steps=math.ceil(CONFIG.log_interval / 10 / CONFIG.grad_accumulation),\n",
    "    accumulate_grad_batches=CONFIG.grad_accumulation,\n",
    "    gradient_clip_val=CONFIG.grad_clip,\n",
    "    profiler=\"simple\",\n",
    "    logger=wandb_logger,\n",
    "    precision=\"16-mixed\",\n",
    "    # callbacks=[lr_monitor, text_gen_callback],\n",
    "    callbacks=[lr_monitor],\n",
    "    # num_sanity_val_steps=0\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CodeNetSentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
