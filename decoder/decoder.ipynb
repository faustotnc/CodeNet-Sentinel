{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# To make our imports work because python relative imports suck\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "# Local Modules\n",
    "from Architecture import PositionalEncoder, Tokenizer, VOCAB_SIZE\n",
    "from Architecture.ModelConfig import ModelConfig\n",
    "from Architecture.Decoder import DecoderDataset, DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = ModelConfig()\n",
    "\n",
    "torch.manual_seed(CONFIG.random_seed)\n",
    "# np.rand\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DecoderDataset.load_from(\"./data/decoder_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(CONFIG.val_split * dataset_size))\n",
    "\n",
    "if CONFIG.shuffle_dataset:\n",
    "    np.random.seed(CONFIG.random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    sampler=val_sampler,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Training Batches:\", len(train_loader))\n",
    "print(\"Number of Validation Batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(val_loader))\n",
    "print(sample_batch.keys(), \"\\n\")\n",
    "\n",
    "for key, item in sample_batch.items():\n",
    "    print(f\"{key}:\".ljust(24), item.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(L.LightningModule):\n",
    "    def __init__(\n",
    "            self, decoder_block,\n",
    "            # Hyperparameters and Config\n",
    "            n_layers, n_head, n_dim, max_seq_len, mlp_dropout, attn_dropout,\n",
    "            vocab_size, learning_rate, min_learning_rate,\n",
    "            weight_decay, beta1, beta2, bias=False, log_interval=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(\n",
    "            ignore_index=Tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        dec = decoder_block(\n",
    "            n_head,\n",
    "            n_dim,\n",
    "            max_seq_len,\n",
    "            mlp_dropout,\n",
    "            attn_dropout,\n",
    "            bias\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = torch.nn.ModuleList(\n",
    "            [copy.deepcopy(dec) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_dim)\n",
    "        self.pos_encoder = PositionalEncoder(n_dim, max_seq_len)\n",
    "        self.final_linear = torch.nn.Linear(n_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, tgt_key_pad_mask, memory=None, memory_key_pad_mask=None):\n",
    "        x = self.pos_encoder(self.embedding(x))\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, tgt_key_pad_mask, memory, memory_key_pad_mask)\n",
    "\n",
    "        logits = self.final_linear(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        _, loss, _ = self._compute_and_log_metrics(batch, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        _, loss, _ = self._compute_and_log_metrics(batch, \"validation\")\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        self._compute_and_log_metrics(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "            betas=(self.hparams.beta1, self.hparams.beta2),\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            patience=10,\n",
    "            min_lr=self.hparams.min_learning_rate,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": self.hparams.log_interval,\n",
    "                \"monitor\": \"validation_loss\",\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _compute_and_log_metrics(self, batch, prefix):\n",
    "        logits = self(batch[\"inputs\"], batch[\"attn_masks\"])\n",
    "        loss = self._compute_loss(logits, batch[\"targets\"])\n",
    "        acc = self._compute_accuracy(logits, batch[\"targets\"])\n",
    "\n",
    "        self.log_dict(\n",
    "            { f\"{prefix}_loss\": loss, f\"{prefix}_accuracy\":  acc },\n",
    "            on_step=True, on_epoch=True, logger=True\n",
    "        )\n",
    "\n",
    "        return logits, loss, acc\n",
    "\n",
    "    def _compute_loss(self, logits, targets):\n",
    "        return self.criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "    def _compute_accuracy(self, logits, targets):\n",
    "        # Get the index of the maximum logit as the predicted token\n",
    "        _, predicted = torch.max(logits, dim=-1)\n",
    "\n",
    "        # Mask out padding positions\n",
    "        non_padding_mask = (targets != Tokenizer.pad_token_id)\n",
    "        total_non_padding = non_padding_mask.sum().item()\n",
    "\n",
    "        correct_predictions = (\n",
    "            predicted[non_padding_mask] == targets[non_padding_mask]\n",
    "        ).sum().item()\n",
    "        \n",
    "        accuracy = correct_predictions / total_non_padding if total_non_padding > 0 else 0.0\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def _generate(self, src, src_pad_mask, tgt_seed, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = tgt_seed[-self.hparams.max_seq_len:]\n",
    "\n",
    "            logits = self(src, idx_cond, src_pad_mask, None)\n",
    "            logits = logits[:, -1]\n",
    "\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            tgt_seed = torch.cat((tgt_seed, idx_next), dim=1)\n",
    "\n",
    "            if idx_next[0][0] == Tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return tgt_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "transformer = DecoderModel(\n",
    "    decoder_block=DecoderBlock,\n",
    "    n_layers=CONFIG.n_layers,\n",
    "    n_head=CONFIG.n_head,\n",
    "    n_dim=CONFIG.n_dim,\n",
    "    max_seq_len=CONFIG.max_seq_len,\n",
    "    mlp_dropout=CONFIG.mlp_dropout,\n",
    "    attn_dropout=CONFIG.attn_dropout,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    learning_rate=CONFIG.learning_rate,\n",
    "    min_learning_rate=CONFIG.min_learning_rate,\n",
    "    weight_decay=CONFIG.weight_decay,\n",
    "    beta1=CONFIG.beta1,\n",
    "    beta2=CONFIG.beta2,\n",
    "    bias=CONFIG.bias,\n",
    "    log_interval=CONFIG.log_interval\n",
    ")\n",
    "\n",
    "# logging\n",
    "if CONFIG.wandb_log:\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=CONFIG.wandb_project_name + \"-decoder\",\n",
    "        name=CONFIG.wandb_run_name,\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "    # log gradients and model topology\n",
    "    wandb_logger.watch(transformer)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\"./checkpoints/\",\n",
    "    max_epochs=CONFIG.num_epochs,\n",
    "    val_check_interval=CONFIG.log_interval,\n",
    "    log_every_n_steps=1,\n",
    "    accumulate_grad_batches=CONFIG.grad_accumulation,\n",
    "    gradient_clip_val=CONFIG.grad_clip,\n",
    "    profiler=\"simple\",\n",
    "    logger=wandb_logger,\n",
    "    precision=\"16-mixed\"\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.fit(\n",
    "    model=transformer,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
